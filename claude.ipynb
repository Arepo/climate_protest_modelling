{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a41ce39-49fb-404d-9bb3-e4e5a7f6ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import (train_test_split, \n",
    "cross_val_score, permutation_test_score, RandomizedSearchCV)\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "import re\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82848972-57f5-4d10-bdb7-6bc5383e8f15",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55275edf-4185-4670-b84c-8b281f9d805b",
   "metadata": {},
   "source": [
    "**Fetch data on European climate protests from 2020 onwards (when it seems to start being somewhat reliably tracked)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fdef15e-55a2-476a-b799-8169083458cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_protests_df = pd.read_csv(\"./data/protests.csv\")\n",
    "og_protests_df.event_date = pd.to_datetime(og_protests_df.event_date)\n",
    "og_protests_df.sort_values(by=\"event_date\", inplace=True)\n",
    "# Protest data from 2019 looks highly suspicious - less than 10% as many\n",
    "# protests as 2020 with around 1/30th or 1/40th as many total protesters\n",
    "og_protests_df = og_protests_df[og_protests_df['event_date'] >= '2020-01-01']\n",
    "og_protests_df = og_protests_df[['event_date', 'tags']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba21f2-1936-4464-9622-dd7d69f321ad",
   "metadata": {},
   "source": [
    "*Note: protest data was taken from the [ACLED database](https://acleddata.com/data-export-tool/) on 2024-07-17, filtering for event_type = \"Protests\", year = 2019 through to 2024, regions=[\"Europe\"], keyword=\"climate\"*\n",
    "\n",
    "*Parliament speech data was taken from the [EU Parliament database](https://data.europarl.europa.eu/en/home); date and parameters unknown*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f82aba-987e-4d7b-a282-d4af632a8aec",
   "metadata": {},
   "source": [
    "**Fetch data on European Parliament speech content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24992320-e955-4efd-b70f-82d86f7884f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df = pd.read_csv('./data/speeches.csv.gz', escapechar=\"\\\\\", compression=\"gzip\")\n",
    "speeches_df[\"date\"] = pd.to_datetime(speeches_df[\"date\"])\n",
    "speeches_df = speeches_df[~speeches_df.date.isna()]\n",
    "speeches_df = speeches_df.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15897e11-dc4f-4b19-8547-000a8b915c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_protest = og_protests_df.event_date.iloc[0]\n",
    "first_speech = speeches_df.date.iloc[0]\n",
    "last_recorded_protest = og_protests_df.event_date.iloc[-1]\n",
    "last_recorded_speech = speeches_df.date.iloc[-1]\n",
    "print(f'First speech: {first_speech}')\n",
    "print(f'First protest: {first_protest}')\n",
    "print(f'Last recorded speech: {last_recorded_speech}')\n",
    "print(f'Last recorded protest: {last_recorded_protest}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52df1cd-195c-4996-bfee-13c69b8dc5dd",
   "metadata": {},
   "source": [
    "**Ensure the dataframes cover the same date range (some protest data is available from slightly earlier, but is highly patchy)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da299e50-ca5b-4384-8545-caa634d40e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop speeches outside the dates for which we have convincing protest data\n",
    "speeches_df = speeches_df[(speeches_df['date'] >= '2020-01-01') & (speeches_df['date'] <= last_recorded_protest)]\n",
    "\n",
    "# Drop protests after the last speech for which we have a complete protest record\n",
    "og_protests_df = og_protests_df[og_protests_df['event_date'] <= speeches_df.date.iloc[-1]]\n",
    "\n",
    "# speeches_df = speeches_df[~speeches_df['date'].isna()]\n",
    "speeches_df = speeches_df.sort_values(by=\"date\")\n",
    "speeches_df = speeches_df[~speeches_df['date'].isna()]\n",
    "speeches_df = speeches_df[~speeches_df['text'].isna()]\n",
    "speeches_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecccba3a-4da8-4594-a6d6-d422e5b74226",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_relevant_protest = og_protests_df.event_date.iloc[0]\n",
    "first_relevant_speech = speeches_df.date.iloc[0]\n",
    "last_relevant_protest = og_protests_df.event_date.iloc[-1]\n",
    "last_relevant_speech = speeches_df.date.iloc[-1]\n",
    "print(f'First relevant speech: {first_relevant_speech}')\n",
    "print(f'First relevant protest: {first_relevant_protest}')\n",
    "print(f'Last relevant speech: {last_relevant_speech}')\n",
    "print(f'Last relevant protest: {last_relevant_protest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89349722-e5bd-48d4-89d3-2b250ddd0fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(speeches_df['text'].iloc[0]))\n",
    "speeches_df['text'].iloc[0][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b39e8bb-a2bf-4266-b2fc-2caa37330806",
   "metadata": {},
   "source": [
    "**Remove prepended metadata from speech transcripts TODO: refine this regex (though it seems to be working surprisingly well for an MVP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651bb30-227c-4589-81e3-dcbc5465a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  pattern = re.compile(r'\\.+.*\\.+') # delete first section with '...'\n",
    "  text = pattern.sub(r'', text)\n",
    "  pattern = re.compile(r\"\\n\") # delete new line markers\n",
    "  text = pattern.sub(r'', text)\n",
    "  text = re.sub(r'\\s+', ' ', text) # delete additional whitespace between words\n",
    "  text = text.lower()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3001ba61-65f5-4b0d-b0e5-d407621236a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df[\"clean_text\"] = speeches_df[\"text\"].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ffdfb-91b9-4c07-b990-eec3dbf978c6",
   "metadata": {},
   "source": [
    "**Combine transcript of multiple speeches on a single day, so we have at most one transcript per day**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd367d-d01e-4011-9cff-cd056642dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the \"text\" column with empty strings\n",
    "speeches_df[\"clean_text\"] = speeches_df[\"clean_text\"].fillna(\"\")\n",
    "\n",
    "# Group by day and aggregate the text\n",
    "speeches_df = speeches_df.groupby(speeches_df[\"date\"]).agg({\"clean_text\": \" \".join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5ed57-9533-4685-8ad1-e3da2fba3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df['speech_length'] = speeches_df[\"clean_text\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d39c2-7a87-4726-8e8a-2c0c23d04341",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(speeches_df['clean_text'].iloc[0]))\n",
    "speeches_df['clean_text'].iloc[0][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea02da-93a3-4b15-952d-d1a707121255",
   "metadata": {},
   "source": [
    "**Get a translation of the phrase 'climate change' for every European language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c52165-ffd6-4ca2-86ef-639d29212794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate keywords to eu-languages\n",
    "keywords = \"climate change\"\n",
    "eu_lang = ['bg','cs','da','de','el','en','es','et','fi','fr','ga','hr','hu','it','lt','lv','mt','nl','pl','pt','ro','sk','sl','sv']\n",
    "keywords_translated = []\n",
    "for i in eu_lang:\n",
    "  translated = GoogleTranslator(source='auto', target=i).translate(text=keywords)\n",
    "  keywords_translated.append(translated)\n",
    "keywords_eu_lang = \"|\".join(keywords_translated).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343cf6f5-fe20-4268-9f73-f8369c16f20f",
   "metadata": {},
   "source": [
    "**Find out what proportion of words in the transcript for each day with at least one speech are (a translation of) the phrase 'climate change' (\"Down with climate change!\" would register as 0.25)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc889987-c32d-405f-a35e-123a62367e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df['climate_mentions_percent'] = (speeches_df[\"clean_text\"].str.count(keywords_eu_lang) / speeches_df[\"clean_text\"].str.split().str.len())*100\n",
    "speeches_df.drop('clean_text', axis=1, inplace=True)\n",
    "speeches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957a59f-88d7-41db-94b3-1575a200c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(22, 5))\n",
    "speeches_df['climate_mentions_percent'].plot(ax=ax, marker='o', linestyle='-', linewidth=2, color=\"#0202cc\", fontsize=18)\n",
    "\n",
    "ax.set_title(\"Proportion of 'Climate Change' Mentions in European Parliament Speeches\", fontsize=20, fontweight = \"bold\", pad=25)\n",
    "ax.set_ylabel(\"Keyword density\",fontsize=18)\n",
    "ax.set_xlabel(\"\")\n",
    "\n",
    "def format_func(value, tick_number):\n",
    "  return '{:.2f} %'.format(value)\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(format_func))\n",
    "\n",
    "gcs_dates = [\"2020-09-25\", \"2021-03-19\", \"2021-09-24\", \"2022-03-25\", \"2022-09-23\", \"2023-03-03\", \"2023-09-15\"]\n",
    "for i in gcs_dates:\n",
    "  plt.axvline(x = i, color = '#b3b3ef', label = 'Global Climate Strikes', linewidth=2)\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.axvline(x = \"2020-03-11\", color = 'pink', label = 'Other political events', linewidth=2)\n",
    "plt.axvline(x = \"2022-02-24\", color = 'pink', label = 'Other political events', linewidth=2)\n",
    "ax.text(\"2020-03-11\", 0.11, \"Start of Covid-19 \\npandemic\", rotation=0, verticalalignment='top', horizontalalignment='center', color=\"red\")\n",
    "ax.text(\"2022-02-24\", 0.11, \"Russian invasion \\nof Ukraine\", rotation=0, verticalalignment='top', horizontalalignment='center', color=\"red\")\n",
    "\n",
    "# Remove legend\n",
    "# ax.legend().remove()\n",
    "# Get legend handles and labels\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend_dict = {label: handle for handle, label in zip(handles, labels)}\n",
    "legend_dict.pop('speech_count', None)\n",
    "\n",
    "# Create legend for each color\n",
    "plt.legend(legend_dict.values(), legend_dict.keys(), loc='upper right', fontsize=\"x-large\")\n",
    "\n",
    "\n",
    "# Rotate x-axis tick labels\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f410b5d-4adc-4dc1-8d6e-c0f6efb003dc",
   "metadata": {},
   "source": [
    "**Convert natural language statements of crowd size at protests into integers. For now, we leave 'unknown' as NAN, and flag them for later processing since there are various things we might decide to do here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589df0b-8278-4b7e-b2e8-07e5abf59236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_crowd_size(tag):\n",
    "  \"\"\"Convert the text strings giving estimated protester counts (sometimes as a\n",
    "  range) into an integer (in the middle of the range)\"\"\"\n",
    "\n",
    "  # Start with the numerous special cases\n",
    "  if any (s in tag for s in (\"several\", \"multiple\", \"a few\", \"some\", \"a group\",\n",
    "    \"a small group\", \"small group\", \"a couple\", \"half dozen\", \"half-dozen\",\n",
    "    \"half a dozen\", \"a handful\", \"handful\", \"small\")\n",
    "  ):\n",
    "    return 5\n",
    "\n",
    "  estimates = {\n",
    "    (\"hundreds\", \"hundred\"): 500,\n",
    "    (\"between hundreds and 2000\",): 2750,\n",
    "    (\"thousands\", \"thousand\"): 5000,\n",
    "    (\"tens of thousands\",): 50_000,\n",
    "    (\"hundreds of thousands\",): 500_000,\n",
    "    (\"between dozens and 250\",): (50+250)//2,\n",
    "    (\"one to three\",): 2,\n",
    "    (\"two dozens\",): 24\n",
    "  }\n",
    "\n",
    "  for text, estimate in estimates.items():\n",
    "    if any (s in tag for s in text):\n",
    "      return estimate\n",
    "\n",
    "  if any (s in tag for s in (\"dozens\", \"tens\", \"tenfold\", \"scores\", \"big group\",\n",
    "    \"large group\", \"numerous\", \"many\", \"large number\", \"numerous small groups\",\n",
    "    \"large\", \"multitude\")):\n",
    "    return 50\n",
    "\n",
    "  # Regex to capture Arabic numbers that may or may not include commas:\n",
    "  number_pattern = '\\d{1,3}(?:,?\\d*)*'\n",
    "  numbers = re.findall(number_pattern, tag)\n",
    "  # Remove commas from numbers and then convert them to integers\n",
    "  numbers = [int(num.replace(',', '')) for num in numbers]\n",
    "\n",
    "  # If we have two or more numbers, assume it's a range, and take their mean\n",
    "  mean = np.mean(numbers)\n",
    "\n",
    "  if tag.endswith(\"dozen\") and any(numbers):\n",
    "    return mean * 12\n",
    "  if tag.endswith(\"dozen\"):\n",
    "    return 12\n",
    "\n",
    "  # Don't replace NaNs yet\n",
    "  if np.isnan(mean):\n",
    "    return np.nan\n",
    "  return int(round(mean))\n",
    "\n",
    "og_protests_df['protesters_estimate'] = og_protests_df['tags'].apply(infer_crowd_size)\n",
    "og_protests_df['size_is_unknown'] = og_protests_df['protesters_estimate'].isna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b751a-a01c-41e1-a9cc-511495860e8f",
   "metadata": {},
   "source": [
    "**Frustratingly, we're missing data on the size of protest for the majority of protests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8234c-a20c-420e-897b-da5ebac684c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(og_protests_df), sum(og_protests_df['size_is_unknown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0182be1-221d-4c47-85c8-dcf63432a3cc",
   "metadata": {},
   "source": [
    "# Model 1: Naive regression, grouping protests by a large regular interval\n",
    "\n",
    "Since we don't have a target value unless there's at least one speech, we could group all our data by regular intervals - but those intervals have to be large enough to reliably contain a speech throughout our data (which turns out to be every 70 days).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7f815-5e76-4e94-8a9c-1a827f7bfd99",
   "metadata": {},
   "source": [
    "**Find the max interval between speeches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738e9bf-e794-4589-b1d6-4a1d6ed1697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.Series(speeches_df.index)\n",
    "\n",
    "intervals = [dates.iloc[i] - dates.iloc[i - 1] for i in range(len(speeches_df)) if i > 0] \n",
    "max_interval = max(intervals)\n",
    "max_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27371f8a-4a2b-4e6b-b34d-ccb4de2efcc3",
   "metadata": {},
   "source": [
    "**Group each protest and each speech into some number of max_intervals (i.e. 70-day periods) from our start time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859de5c-1418-4f5b-b482-8c1a990be1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_intervals_from_start(date):\n",
    "    start_date = og_protests_df['event_date'].min()\n",
    "    offset = pd.offsets.Day(max_interval.days)\n",
    "    period = start_date.to_period(freq=offset)\n",
    "    intervals = (date - start_date) // max_interval\n",
    "    return period + intervals + 1\n",
    "\n",
    "# Create a new column with the periods\n",
    "speeches_df['grouping_period'] = speeches_df.index.map(count_intervals_from_start)\n",
    "\n",
    "# Group by the period\n",
    "y_regular = speeches_df.groupby('grouping_period')['climate_mentions_percent'].mean()\n",
    "y_regular.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83c18d-19b0-477c-bdc0-5946f33cb316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the protests by the same interval as above\n",
    "og_protests_df['grouping_period'] = og_protests_df['event_date'].map(count_intervals_from_start)\n",
    "\n",
    "num_protests = og_protests_df.groupby('grouping_period').size()\n",
    "\n",
    "regularised_protests_df = og_protests_df.groupby('grouping_period').agg({\n",
    "    'protesters_estimate': 'sum',\n",
    "    'size_is_unknown': 'sum'\n",
    "}).assign(num_protests=og_protests_df.groupby('grouping_period').size())\n",
    "\n",
    "regularised_protests_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f17f50-4c6f-4856-bc41-cf25d08aa183",
   "metadata": {},
   "source": [
    "**For our basic model, we treat each 70-day period as unrelated, with protests having no persistent effects beyond the subsequent speech. We also assume the mean number of protesters for those with missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17779e-de47-4ff5-80d3-6e3fa7db7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtestSizeImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['protesters_estimate'] = X['protesters_estimate'] + X['size_is_unknown'] * og_protests_df['protesters_estimate'].mean()\n",
    "        return X['protesters_estimate']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b793fb-b3ec-4d42-8d12-c1c411601377",
   "metadata": {},
   "source": [
    "**There's already strong and robust correlations between [number of *protests* & climate mentions] and between [number of protest*ers* & climate mentions]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5eb838-4511-4a18-9775-134399afd4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtestSizeImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['protesters_estimate'] = X['protesters_estimate'] + X['size_is_unknown'] * og_protests_df['protesters_estimate'].mean()\n",
    "        return X[['protesters_estimate']]\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('protest_size_imputer', ProtestSizeImputer(), ['protesters_estimate', 'size_is_unknown']),\n",
    "        ('retained', 'passthrough', ['num_protests'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "protester_transformer = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "transformed_data = preprocessor.fit_transform(regularised_protests_df)\n",
    "\n",
    "output_column_names = ['protesters_estimate', 'num_protests']\n",
    "\n",
    "# Create a DataFrame with the transformed data and correct column names\n",
    "correlation_df = pd.DataFrame(transformed_data, columns=output_column_names, index=regularised_protests_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa185b3e-7c02-4de5-aa61-0c7da1598209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_test_df = ProtestSizeImputer().fit_transform(regularised_protests_df[['protesters_estimate', 'size_is_unknown']])\n",
    "corr_test_df['climate_mentions'] = y_regular\n",
    "corr_test_df = corr_test_df.reset_index(drop=True)\n",
    "corr_test_df\n",
    "sns.heatmap(corr_test_df.corr(), annot=True, cmap='viridis')\n",
    "plt.title(\"Correlation Heatmap\", fontsize=16);\n",
    "cross_val_score(LinearRegression(), corr_test_df[['protesters_estimate']], corr_test_df['climate_mentions'], scoring='r2').mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
